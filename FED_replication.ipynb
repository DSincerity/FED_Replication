{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import copy\n",
    "\n",
    "with open('fed_data.json', 'r') as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"User: Hi!\\nSystem: Hi! What's up?\\nUser: Nothing much, how about you\\nSystem: Not much either.\\nUser: What are you doing\\nSystem: Playing Terraria. What about you?\\nUser: Sitting in a meeting\\nSystem: What kind of meeting?\\nUser: Can't say\",\n",
       " 'response': \"System: It's probably boring, isn't it?\",\n",
       " 'system': 'Meena',\n",
       " 'annotations': {'Interesting': [2, 1, 1, 1, 1],\n",
       "  'Engaging': [2, 1, 2, 2, 2],\n",
       "  'Specific': [1, 1, 2, 2, 1],\n",
       "  'Relevant': [2, 1, 2, 2, 2],\n",
       "  'Correct': [2, 2, 1, 2, 2],\n",
       "  'Semantically appropriate': [2, 1, 2, 2, 2],\n",
       "  'Understandable': [1, 1, 1, 1, 1],\n",
       "  'Fluent': [2, 1, 2, 2, 2],\n",
       "  'Overall': [2, 1, 3, 3, 4]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn-level 123 120 132\n",
      "Dialog-level 41 40 44\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into turn-level and dialol-level\n",
    "Meena_cnt=0\n",
    "Human_cnt=0\n",
    "Mitsuki_cnt=0\n",
    "Meena_session_cnt=0\n",
    "Human_session_cnt=0\n",
    "Mitsuki_session_cnt=0\n",
    "dialog_cnt=0\n",
    "turn_level = []\n",
    "dialog_level = []\n",
    "\n",
    "for x in data:\n",
    "    try:\n",
    "        \n",
    "        res= x['response']\n",
    "        if x['system'] =='Meena':\n",
    "            Meena_cnt +=1\n",
    "        elif x['system'] =='Human':\n",
    "            Human_cnt+=1\n",
    "        elif x['system'] =='Mitsuku':\n",
    "            Mitsuki_cnt +=1\n",
    "        else:\n",
    "            print(x)\n",
    "        turn_level.append(x)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        dialog_cnt +=1\n",
    "        if x['system'] =='Meena':\n",
    "            Meena_session_cnt +=1\n",
    "        elif x['system'] =='Human':\n",
    "            Human_session_cnt+=1\n",
    "        elif x['system'] =='Mitsuku':\n",
    "            Mitsuki_session_cnt +=1\n",
    "        else:\n",
    "            print('session cnt', x)\n",
    "        dialog_level.append(x)\n",
    "        \n",
    "print('Turn-level', Human_cnt, Meena_cnt, Mitsuki_cnt)       \n",
    "print('Dialog-level', Human_session_cnt, Meena_session_cnt, Mitsuki_session_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the systems based on annotator's evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Interesting', 'Engaging', 'Specific', 'Relevant', 'Correct', 'Semantically appropriate', 'Understandable', 'Fluent', 'Overall']\n",
      "\n",
      "{'Interesting': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Engaging': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Specific': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Relevant': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Correct': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Semantically appropriate': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Understandable': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Fluent': {'Human': [], 'Meena': [], 'Mitsuku': []}, 'Overall': {'Human': [], 'Meena': [], 'Mitsuku': []}}\n"
     ]
    }
   ],
   "source": [
    "# Set final output\n",
    "turn_attributes =  list(turn_level[0]['annotations'].keys())\n",
    "turn_evaluation = { attribute: {\"Human\": [], \"Meena\": [], \"Mitsuku\": []} for attribute in turn_attributes}\n",
    "print(turn_attributes)\n",
    "print()\n",
    "print(turn_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "import numpy as np \n",
    "import copy\n",
    "\n",
    "def remove_furthest(scores):\n",
    "    score_list = copy.deepcopy(scores)\n",
    "    mean= round(np.mean(score_list),3)\n",
    "    std= round(np.std(score_list), 3)\n",
    "    score_list= sorted(score_list, key=lambda x: abs(x-mean), reverse=True)  # reorder based on distance\n",
    "    remove_idx=-1\n",
    "    for idx, x in enumerate(score_list):\n",
    "        if abs(x-mean) > std/2:\n",
    "            remove_idx=idx\n",
    "            break\n",
    "    if remove_idx != -1:\n",
    "        del score_list[remove_idx] \n",
    "    return score_list\n",
    "\n",
    "\n",
    "turn_attributes =  list(turn_level[0]['annotations'].keys())\n",
    "turn_evaluation = { attribute: {\"Human\": [], \"Meena\": [], \"Mitsuku\": []} for attribute in turn_attributes}\n",
    "\n",
    "for turn in turn_level:\n",
    "    for attr in turn_attributes:\n",
    "        scores= turn['annotations'][attr]\n",
    "        filterd_list = list(filter(lambda e: isinstance(e, int), scores))  # remove texts in annotators' evaluation\n",
    "        removed_furthest = remove_furthest(filterd_list)  # remove furthest label\n",
    "        turn_evaluation[attr][turn['system']].append(np.mean(removed_furthest))\n",
    "        \n",
    "turn_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting Human 123\n",
      "Interesting Meena 120\n",
      "Interesting Mitsuku 132\n",
      "Engaging Human 123\n",
      "Engaging Meena 120\n",
      "Engaging Mitsuku 132\n",
      "Specific Human 123\n",
      "Specific Meena 120\n",
      "Specific Mitsuku 132\n",
      "Relevant Human 123\n",
      "Relevant Meena 120\n",
      "Relevant Mitsuku 132\n",
      "Correct Human 123\n",
      "Correct Meena 120\n",
      "Correct Mitsuku 132\n",
      "Semantically appropriate Human 123\n",
      "Semantically appropriate Meena 120\n",
      "Semantically appropriate Mitsuku 132\n",
      "Understandable Human 123\n",
      "Understandable Meena 120\n",
      "Understandable Mitsuku 132\n",
      "Fluent Human 123\n",
      "Fluent Meena 120\n",
      "Fluent Mitsuku 132\n",
      "Overall Human 123\n",
      "Overall Meena 120\n",
      "Overall Mitsuku 132\n"
     ]
    }
   ],
   "source": [
    "# double-check the counts of turns\n",
    "for attr in turn_attributes:\n",
    "    for system in [\"Human\",\"Meena\",\"Mitsuku\"]:\n",
    "        print(attr, system, len(turn_evaluation[attr][system]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>Mitsuku</th>\n",
       "      <th>Meena</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.418750</td>\n",
       "      <td>2.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engaging</td>\n",
       "      <td>2.331439</td>\n",
       "      <td>2.581250</td>\n",
       "      <td>2.837398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specific</td>\n",
       "      <td>2.371843</td>\n",
       "      <td>2.558333</td>\n",
       "      <td>2.821138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>2.535354</td>\n",
       "      <td>2.922917</td>\n",
       "      <td>2.898374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Correct</td>\n",
       "      <td>2.419192</td>\n",
       "      <td>2.884722</td>\n",
       "      <td>2.894309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semantically appropriate</td>\n",
       "      <td>2.679924</td>\n",
       "      <td>2.952083</td>\n",
       "      <td>2.896341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Understandable</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fluent</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>2.956250</td>\n",
       "      <td>2.832656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Overall</td>\n",
       "      <td>3.285985</td>\n",
       "      <td>4.179167</td>\n",
       "      <td>4.321138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Quality   Mitsuku     Meena     Human\n",
       "1               Interesting  2.121212  2.418750  2.713415\n",
       "2                  Engaging  2.331439  2.581250  2.837398\n",
       "3                  Specific  2.371843  2.558333  2.821138\n",
       "4                  Relevant  2.535354  2.922917  2.898374\n",
       "5                   Correct  2.419192  2.884722  2.894309\n",
       "6  Semantically appropriate  2.679924  2.952083  2.896341\n",
       "7            Understandable  0.958333  1.000000  0.987805\n",
       "8                    Fluent  2.812500  2.956250  2.832656\n",
       "9                   Overall  3.285985  4.179167  4.321138"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance (Avg score of annotators' scores)\n",
    "# Turn-level\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.DataFrame(columns=['Quality','Human','Meena', 'Mitsuku'])\n",
    "idx=0\n",
    "for attr, model_eval in turn_evaluation.items():\n",
    "    idx +=1\n",
    "    df_row= [attr] \n",
    "    \n",
    "    for model, scores in model_eval.items():\n",
    "        avg = np.mean(scores)\n",
    "        \n",
    "        # adjust score range\n",
    "        if attr not in ('Understandable', 'Consistent'): \n",
    "            avg += 1       \n",
    "        df_row += [avg]   \n",
    "    df.loc[idx] = df_row\n",
    "\n",
    "df[['Quality','Mitsuku', 'Meena', 'Human']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>Mitsuku</th>\n",
       "      <th>Meena</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>2.418750</td>\n",
       "      <td>2.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engaging</td>\n",
       "      <td>2.331439</td>\n",
       "      <td>2.581250</td>\n",
       "      <td>2.837398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specific</td>\n",
       "      <td>2.371843</td>\n",
       "      <td>2.558333</td>\n",
       "      <td>2.821138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>2.535354</td>\n",
       "      <td>2.922917</td>\n",
       "      <td>2.898374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Correct</td>\n",
       "      <td>2.419192</td>\n",
       "      <td>2.884722</td>\n",
       "      <td>2.894309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semantically appropriate</td>\n",
       "      <td>2.679924</td>\n",
       "      <td>2.952083</td>\n",
       "      <td>2.896341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Understandable</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fluent</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>2.956250</td>\n",
       "      <td>2.832656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Overall</td>\n",
       "      <td>3.285985</td>\n",
       "      <td>4.179167</td>\n",
       "      <td>4.321138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Quality   Mitsuku     Meena     Human\n",
       "1               Interesting  2.121212  2.418750  2.713415\n",
       "2                  Engaging  2.331439  2.581250  2.837398\n",
       "3                  Specific  2.371843  2.558333  2.821138\n",
       "4                  Relevant  2.535354  2.922917  2.898374\n",
       "5                   Correct  2.419192  2.884722  2.894309\n",
       "6  Semantically appropriate  2.679924  2.952083  2.896341\n",
       "7            Understandable  0.958333  1.000000  0.987805\n",
       "8                    Fluent  2.812500  2.956250  2.832656\n",
       "9                   Overall  3.285985  4.179167  4.321138"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance (Avg score of annotators' scores)\n",
    "# Turn-level\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.DataFrame(columns=['Quality','Human','Meena', 'Mitsuku'])\n",
    "idx=0\n",
    "for attr, model_eval in turn_evaluation.items():\n",
    "    idx +=1\n",
    "    df_row= [attr] \n",
    "    \n",
    "    for model, scores in model_eval.items():\n",
    "        avg = np.mean(scores)\n",
    "        \n",
    "        # adjust score range\n",
    "        if attr not in ('Understandable', 'Consistent'): \n",
    "            avg += 1       \n",
    "        df_row += [avg]   \n",
    "    df.loc[idx] = df_row\n",
    "\n",
    "df[['Quality','Mitsuku', 'Meena', 'Human']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No score case:  ['N/A (no mistakes made)', 'N/A (The system did not make any errors.)', 'N/A (There were no errors)', 'N/A (No real errors to recover from)', 'N/A (no errors)'] []\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance (Avg score of annotators' scores)\n",
    "# Dialog -level\n",
    "\n",
    "import numpy as np \n",
    "dialog_attributes =  list(dialog_level[0]['annotations'].keys())\n",
    "dialog_evaluation= { attribute: {\"Human\": [], \"Meena\": [], \"Mitsuku\": []} for attribute in dialog_attributes}\n",
    "\n",
    "for dialog in dialog_level:\n",
    "    for attr in dialog_attributes:\n",
    "        scores= dialog['annotations'][attr]\n",
    "        filterd_list = list(filter(lambda e: isinstance(e, int), scores))\n",
    "        removed_furthest = remove_furthest(filterd_list)\n",
    "        if len(removed_furthest) == 0:  # set a score for empty lists due to only text evaluation. \n",
    "            print('No score case: ', scores, removed_furthest)\n",
    "            removed_furthest=[2]\n",
    "        dialog_evaluation[attr][dialog['system']].append(np.mean(removed_furthest))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>Mitsuku</th>\n",
       "      <th>Meena</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coherent</td>\n",
       "      <td>2.193182</td>\n",
       "      <td>2.887500</td>\n",
       "      <td>2.945122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Error recovery</td>\n",
       "      <td>2.219697</td>\n",
       "      <td>2.697917</td>\n",
       "      <td>2.869919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consistent</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.987805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diverse</td>\n",
       "      <td>2.255682</td>\n",
       "      <td>2.481250</td>\n",
       "      <td>2.884146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Depth</td>\n",
       "      <td>1.795455</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Likeable</td>\n",
       "      <td>2.130682</td>\n",
       "      <td>2.637500</td>\n",
       "      <td>2.969512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Understanding</td>\n",
       "      <td>2.227273</td>\n",
       "      <td>2.862500</td>\n",
       "      <td>2.981707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flexible</td>\n",
       "      <td>2.238636</td>\n",
       "      <td>2.706250</td>\n",
       "      <td>2.969512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Informative</td>\n",
       "      <td>2.085227</td>\n",
       "      <td>2.587500</td>\n",
       "      <td>2.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Inquisitive</td>\n",
       "      <td>2.335227</td>\n",
       "      <td>2.768750</td>\n",
       "      <td>2.884146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overall</td>\n",
       "      <td>3.102273</td>\n",
       "      <td>4.112500</td>\n",
       "      <td>4.603659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Quality   Mitsuku     Meena     Human\n",
       "1         Coherent  2.193182  2.887500  2.945122\n",
       "2   Error recovery  2.219697  2.697917  2.869919\n",
       "3       Consistent  0.886364  0.968750  0.987805\n",
       "4          Diverse  2.255682  2.481250  2.884146\n",
       "5            Depth  1.795455  2.250000  2.780488\n",
       "6         Likeable  2.130682  2.637500  2.969512\n",
       "7    Understanding  2.227273  2.862500  2.981707\n",
       "8         Flexible  2.238636  2.706250  2.969512\n",
       "9      Informative  2.085227  2.587500  2.853659\n",
       "10     Inquisitive  2.335227  2.768750  2.884146\n",
       "11         Overall  3.102273  4.112500  4.603659"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalize ouputs\n",
    "df= pd.DataFrame(columns=['Quality','Human','Meena', 'Mitsuku'])\n",
    "idx=0\n",
    "for attr, model_eval in dialog_evaluation.items():\n",
    "    idx +=1\n",
    "    df_row= [attr] \n",
    "    \n",
    "    for model, scores in model_eval.items():\n",
    "        avg = np.mean(scores)\n",
    "        \n",
    "        # adjust score range (0~2 -> 1~3)\n",
    "        if attr not in ('Understandable', 'Consistent'): \n",
    "            avg += 1\n",
    "        df_row += [avg]\n",
    "    df.loc[idx] = df_row\n",
    "\n",
    "df[['Quality','Mitsuku', 'Meena', 'Human']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Human annotations\n",
    "\n",
    "import numpy as np \n",
    "turn_attributes =  list(turn_level[0]['annotations'].keys())\n",
    "turn_evaluation = { attribute: []  for attribute in turn_attributes}\n",
    "\n",
    "for turn in turn_level:\n",
    "    for attr in turn_attributes:\n",
    "        scores= turn['annotations'][attr]\n",
    "        filterd_list = list(filter(lambda e: isinstance(e, int), scores))\n",
    "        removed_furthest = remove_furthest(filterd_list)\n",
    "        avg= np.mean(removed_furthest)\n",
    "        \n",
    "        if attr not in ('Understandable', 'Consistent'): \n",
    "            avg += 1\n",
    "        turn_evaluation[attr].append(avg)\n",
    "        #print(scores, removed_furthest)\n",
    "turn_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.9.2/envs/lt/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:588: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "import fed\n",
    "\n",
    "model, tokenizer = fed.load_models(\"microsoft/DialoGPT-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model FU Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocessing_conversation(context:str):\n",
    "    preproc_context= \" \".join([utter.replace('User:', \"<|endoftext|>\").replace('System:', \"<|endoftext|>\") for utter in context['context'].split('\\n')])\n",
    "    response= context['response'].replace('System:', \"<|endoftext|>\")\n",
    "\n",
    "    return f\"{preproc_context} {response}\"\n",
    "\n",
    "def save(f, kwarg):\n",
    "    assert kwarg, \"No contents\"\n",
    "\n",
    "    f.write(json.dumps(kwarg, indent=4, ensure_ascii=False))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "# get fed scores\n",
    "turn_model_scores = { attribute: []  for attribute in turn_attributes if attribute != 'Overall'}\n",
    "output_file='turn_level_FU_scores.jsonl'\n",
    "with open(output_file, \"w\") as ofile:\n",
    "\n",
    "    for idx, t in enumerate(tqdm(turn_level)):\n",
    "        context= \" \".join([utter.replace('User:', \"<|endoftext|>\").replace('System:', \"<|endoftext|>\") for utter in t['context'].split('\\n')])\n",
    "        response= t['response'].replace('System:', \"<|endoftext|>\")\n",
    "        conversation_for_eval = context + \" \" +response\n",
    "        scores = fed.evaluate(conversation_for_eval,\n",
    "                            model,\n",
    "                            tokenizer)\n",
    "\n",
    "        for attr in turn_model_scores.keys():\n",
    "            turn_model_scores[attr].append(scores[attr.lower()])\n",
    "        \n",
    "        # logs for checking inputs & outputs\n",
    "        save(ofile, {'idx': idx, 'level': 'Turn-level', 'input':conversation_for_eval, \"score\": scores })\n",
    "turn_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "import json\n",
    "with open(\"turn_level_model_score_list.json\", \"w\") as json_file:\n",
    "    json.dump(turn_model_scores, json_file)\n",
    "\n",
    "with open(\"turn_level_annotators_scores.json\", \"w\") as json_file2:\n",
    "    json.dump(dialog_evaluation, json_file2, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.3126494294346008, pvalue=6.013519364186905e-10)\n",
      "SpearmanrResult(correlation=0.15131290893596858, pvalue=0.0033107299575460295)\n",
      "SpearmanrResult(correlation=0.1933698301956955, pvalue=0.0001647519959582658)\n",
      "SpearmanrResult(correlation=0.15254449277498106, pvalue=0.003061442563010448)\n",
      "SpearmanrResult(correlation=0.16028888712516604, pvalue=0.0018471022318570082)\n",
      "SpearmanrResult(correlation=0.04570597460483709, pvalue=0.37745303519693696)\n",
      "SpearmanrResult(correlation=0.024830238839847352, pvalue=0.6317222591672215)\n",
      "SpearmanrResult(correlation=0.011663739703123023, pvalue=0.8218851773786329)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>Pearson-Corr</th>\n",
       "      <th>Spearman-Corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interesting</td>\n",
       "      <td>0.279838</td>\n",
       "      <td>0.312649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engaging</td>\n",
       "      <td>0.143199</td>\n",
       "      <td>0.151313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Specific</td>\n",
       "      <td>0.182277</td>\n",
       "      <td>0.193370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>0.152544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Correct</td>\n",
       "      <td>0.175289</td>\n",
       "      <td>0.160289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Semantically appropriate</td>\n",
       "      <td>0.083279</td>\n",
       "      <td>0.045706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Understandable</td>\n",
       "      <td>-0.027571</td>\n",
       "      <td>0.024830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fluent</td>\n",
       "      <td>0.038842</td>\n",
       "      <td>0.011664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Quality  Pearson-Corr  Spearman-Corr\n",
       "0               Interesting      0.279838       0.312649\n",
       "1                  Engaging      0.143199       0.151313\n",
       "2                  Specific      0.182277       0.193370\n",
       "3                  Relevant      0.146903       0.152544\n",
       "4                   Correct      0.175289       0.160289\n",
       "5  Semantically appropriate      0.083279       0.045706\n",
       "6            Understandable     -0.027571       0.024830\n",
       "7                    Fluent      0.038842       0.011664"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate correlation and preprocess it for an output format\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "\n",
    "df= pd.DataFrame(columns=['Quality','Pearson-Corr','Spearman-Corr'])\n",
    "idx=0\n",
    "for attr in turn_model_scores.keys():\n",
    "    print(stats.spearmanr(turn_evaluation[attr], turn_model_scores[attr]))\n",
    "    p_corr, _ = stats.pearsonr(turn_evaluation[attr], turn_model_scores[attr])\n",
    "    s_corr, _ = stats.spearmanr(turn_evaluation[attr], turn_model_scores[attr])\n",
    "    df.loc[idx] = [attr, p_corr, s_corr]\n",
    "    idx+=1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog level\n",
    "import numpy as np \n",
    "\n",
    "dialog_attributes =  list(dialog_level[0]['annotations'].keys())\n",
    "dialog_evaluation= { attribute: []  for attribute in dialog_attributes}\n",
    "\n",
    "for dialog in dialog_level:\n",
    "    for attr in dialog_attributes:\n",
    "        scores= dialog['annotations'][attr]\n",
    "        filterd_list = list(filter(lambda e: isinstance(e, int), scores))\n",
    "        removed_furthest = remove_furthest(filterd_list)\n",
    "        if len(removed_furthest) ==0:\n",
    "            removed_furthest=[2] # set a score for empty lists due to only text evaluation\n",
    "        avg= np.mean(removed_furthest)\n",
    "        if attr not in ('Understandable', 'Consistent'): \n",
    "            avg += 1\n",
    "        dialog_evaluation[attr].append(avg)\n",
    "dialog_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model FU Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "dialog_model_scores = { attribute: []  for attribute in dialog_attributes if attribute != 'Overall'}\n",
    "output_file='dialog_level_FU_scores.jsonl'\n",
    "with open(output_file, \"w\") as ofile:\n",
    "\n",
    "    for idx, d in enumerate(tqdm(dialog_level)):\n",
    "        context= \" \".join([utter.replace('User:', \"<|endoftext|>\").replace('System:', \"<|endoftext|>\") for utter in d['context'].split('\\n')])\n",
    "        scores = fed.evaluate(context,\n",
    "                            model,\n",
    "                            tokenizer)\n",
    "\n",
    "        for attr in dialog_model_scores.keys():\n",
    "            if attr == 'Understanding':\n",
    "                dialog_model_scores[attr].append(scores['understand'])   \n",
    "                continue \n",
    "            dialog_model_scores[attr].append(scores[attr.lower()])\n",
    "\n",
    "        save(ofile, {'idx': idx, 'level': 'Dialog-level', 'input':context, \"score\": scores })\n",
    "\n",
    "dialog_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "import json\n",
    "\n",
    "with open(\"dialog_level_model_scores.json\", \"w\") as json_file:\n",
    "    json.dump(dialog_model_scores, json_file)\n",
    "\n",
    "with open(\"dialog_level_annotators_scores.json\", \"w\") as json_file2:\n",
    "    json.dump(dialog_evaluation, json_file2, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>Pearson-Corr</th>\n",
       "      <th>Spearman-Corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coherent</td>\n",
       "      <td>0.073547</td>\n",
       "      <td>0.171332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Error recovery</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.026153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consistent</td>\n",
       "      <td>-0.043399</td>\n",
       "      <td>0.034835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diverse</td>\n",
       "      <td>0.122052</td>\n",
       "      <td>0.079674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depth</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.087264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Likeable</td>\n",
       "      <td>0.204696</td>\n",
       "      <td>0.264306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Understanding</td>\n",
       "      <td>0.177501</td>\n",
       "      <td>0.173499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flexible</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>0.112024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Informative</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.249932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inquisitive</td>\n",
       "      <td>0.176394</td>\n",
       "      <td>0.150219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Quality  Pearson-Corr  Spearman-Corr\n",
       "0        Coherent      0.073547       0.171332\n",
       "1  Error recovery      0.025022       0.026153\n",
       "2      Consistent     -0.043399       0.034835\n",
       "3         Diverse      0.122052       0.079674\n",
       "4           Depth      0.098246       0.087264\n",
       "5        Likeable      0.204696       0.264306\n",
       "6   Understanding      0.177501       0.173499\n",
       "7        Flexible      0.042839       0.112024\n",
       "8     Informative      0.190083       0.249932\n",
       "9     Inquisitive      0.176394       0.150219"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate correlation\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "\n",
    "df2= pd.DataFrame(columns=['Quality','Pearson-Corr','Spearman-Corr'])\n",
    "idx=0\n",
    "for attr in dialog_model_scores.keys():\n",
    "    #print(stats.spearmanr(dialog_model_scores[attr], dialog_model_scores[attr]))\n",
    "    p_corr, _ = stats.pearsonr(dialog_evaluation[attr], dialog_model_scores[attr])\n",
    "    s_corr, _ = stats.spearmanr(dialog_evaluation[attr], dialog_model_scores[attr])\n",
    "    df2.loc[idx] = [attr, p_corr, s_corr]\n",
    "    idx+=1\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edcc098ac14e989ce3f7440a18c9796f0d189647cbdaf6011424eb3ad9ae9224"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('lt': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
